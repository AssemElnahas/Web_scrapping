import requests
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st


# ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
def classify_article(title):
    title = title.lower()
    categories = {
        "ğŸ“Š Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ & Ø§Ù„Ø£Ø¹Ù…Ø§Ù„": ["market", "business", "finance", "stock"],
        "ğŸ¤– Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["AI", "tech", "software", "robotics"],
        "ğŸ¥ Ø§Ù„ØµØ­Ø©": ["health", "medicine", "wellness", "covid"],
    }
    for category, keywords in categories.items():
        if any(keyword in title for keyword in keywords):
            return category
    return "ğŸŒ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø©"


# Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.title("ğŸ” Web Scraping & Data Classification")

# Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø±Ø§Ø¨Ø·
url = st.text_input("ğŸ”— Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹:")

if st.button("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ğŸš€"):
    if not url:
        st.error("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹!")
    else:
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            articles = soup.find_all("article")

            if not articles:
                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø¬Ø±Ù‘Ø¨ Ø±Ø§Ø¨Ø·Ù‹Ø§ Ø¢Ø®Ø±.")
            else:
                data = []
                for article in articles:
                    title = (
                        article.find("h2").text
                        if article.find("h2")
                        else "No Title"
                    )
                    link = (
                        article.find("a")["href"]
                        if article.find("a")
                        else "No Link"
                    )
                    summary = (
                        article.find("p").text
                        if article.find("p")
                        else "No Summary"
                    )
                    category = classify_article(title)

                    data.append({"Title": title, "Link": link,
                                 "Summary": summary, "Category": category})

                # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù CSV
                df = pd.DataFrame(data)
                filename = "classified_scraped_data.csv"
                df.to_csv(filename, index=False, encoding="utf-8")

                # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙ
                st.success(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(data)} Ù…Ù‚Ø§Ù„ ÙˆØªØµÙ†ÙŠÙÙ‡!")
                st.dataframe(df)

                # Ø²Ø± Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
                st.download_button(
                    label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµÙ†ÙØ© ÙƒÙ€ CSV",
                    data=df.to_csv(index=False).encode("utf-8"),
                    file_name="classified_scraped_data.csv",
                    mime="text/csv",
                )

        except requests.exceptions.RequestException as e:
            st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n{e}")
